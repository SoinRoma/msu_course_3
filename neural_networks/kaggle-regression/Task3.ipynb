{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соревнование на kaggle\n",
    "\n",
    "По набору примеров многомерных признаков trainX и правильных ответов trainY из обучающего множества обучить модель регрессии, которую затем необходимо будет применить к набору примеров признаков из тестового множества testX и прислать файл с полученными ответами.\n",
    "Сами признаки никакого осмысленного значения не несут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Первая реализация. Результат ровный baseline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_X = pd.read_csv(\"trainX.csv\", index_col = 'Id').to_numpy()\n",
    "train_Y = pd.read_csv(\"trainY.csv\", index_col = 'Id').to_numpy()\n",
    "test_X = pd.read_csv(\"testX.csv\", index_col = 'Id').to_numpy()\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(train_X, train_Y)\n",
    "\n",
    "y = model.predict(test_X)\n",
    "y = y.reshape(2000,)\n",
    "Ypd = pd.DataFrame({'Value': y, 'Id': range(len(y))})\n",
    "# Ypd.to_csv('soin_roman.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Вторая реализация \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_X = pd.read_csv(\"trainX.csv\", index_col = 'Id').to_numpy()\n",
    "train_Y = pd.read_csv(\"trainY.csv\", index_col = 'Id').to_numpy()\n",
    "test_X = pd.read_csv(\"testX.csv\", index_col = 'Id').to_numpy()\n",
    "\n",
    "class Linear_regression(object):\n",
    "    \n",
    "    def __init__(self, sgd, n_epoch, batch_size):\n",
    "        self.sgd=sgd\n",
    "        self.n_epoch=n_epoch\n",
    "        self.batch_size=batch_size\n",
    "        self.W = None\n",
    "        self.trainX = None\n",
    "        self.trainY = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.trainX = np.array(X)\n",
    "        self.trainY = np.array(y)\n",
    "        self.W = np.random.rand(self.trainX.shape[1])\n",
    "        start = 0\n",
    "        end = self.batch_size\n",
    "        for i in range(self.n_epoch):\n",
    "            while end < self.trainX.shape[0]:\n",
    "                self.W = self.sgd.step(self.trainX[start:end], self.trainY, self.W)\n",
    "                start += self.batch_size\n",
    "                end += self.batch_size\n",
    "            np.random.shuffle(self.trainX)\n",
    "            start = 0\n",
    "            end = self.batch_size\n",
    "                          \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.array(X)\n",
    "        n = X.shape[0]\n",
    "        y = np.array([ self.W @ X[i] for i in range(n)])\n",
    "        return y\n",
    "\n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, grad, alpha):\n",
    "        self.grad=grad\n",
    "        self.alpha=alpha \n",
    "     \n",
    "    def step(self, X, y, W):\n",
    "        \n",
    "        trainX=np.array(X)\n",
    "        k=trainX.shape[0]\n",
    "        new_weights = np.copy(W)\n",
    "        S=0\n",
    "        for i in range(k):\n",
    "            S=S+self.grad.grad(X[i],y[i],W)   \n",
    "        new_weights=new_weights-self.alpha/k*S\n",
    "        return  new_weights\n",
    "       \n",
    "class Grad(object):\n",
    "   \n",
    "    def __init__(self, loss, delta):\n",
    "        self.loss = loss\n",
    "        self.delta = delta\n",
    "        \n",
    "    def grad(self, X, y, W):\n",
    "        \n",
    "        n_weights=W.size\n",
    "        g=np.zeros(n_weights,dtype='float64')\n",
    "        lost = self.loss.val\n",
    "        for i in range(n_weights):\n",
    "            increment=np.zeros(n_weights,dtype='float64')\n",
    "            increment[i]=1 \n",
    "            g[i]=(lost(X,y,W+increment*self.delta) -lost(X,y,W))/self.delta     \n",
    "        return g\n",
    "\n",
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self, l1_coef):\n",
    "        self.l1_coef=l1_coef\n",
    "            \n",
    "    def val(self, X, y, W):\n",
    "       \n",
    "        L1 = (W @ X - y)**2\n",
    "        lost=0\n",
    "        for i in W:\n",
    "            lost += np.abs(i)\n",
    "        lost=lost*self.l1_coef \n",
    "        lost=lost+L1\n",
    "        return lost\n",
    "        \n",
    "def main(model, train_X, train_Y, test_X):\n",
    "    model.fit(train_X, train_Y)\n",
    "    y_pred = model.predict(test_X)\n",
    "    y_pred = y_pred.reshape(2000,)\n",
    "    Ypd = pd.DataFrame({'Value': y_pred, 'Id': range(len(y_pred))})\n",
    "    Ypd.to_csv('soin_roman.csv', index=False)\n",
    "        \n",
    "#Подбор коэффицентов        \n",
    "alpha = 0.0431\n",
    "delta = 0.000000001\n",
    "l1_coef = 0.1757\n",
    "n_epoch = 2\n",
    "batch_size = 9\n",
    "\n",
    "loss = Loss(l1_coef)\n",
    "grad = Grad(loss, delta)\n",
    "sgd = SGD(grad, alpha)\n",
    "\n",
    "model = Linear_regression(sgd, n_epoch, batch_size)\n",
    "main(model, train_X, train_Y, test_X)\n",
    "\n",
    "# Журнал изменений параметров и точность при них\n",
    "# При вервой реализации точность была ровно baseline\n",
    "\n",
    "# При второй реализации и таких параметрах:\n",
    "# alpha = 0.0331\n",
    "# delta = 0.000000001\n",
    "# l1_coef = 0.1747\n",
    "# n_epoch = 2\n",
    "# batch_size = 8\n",
    "# Точность стала 0.61261\n",
    "\n",
    "# Потом на этих же параметрах 0.56036\n",
    "# Точность снизилась. Наверное сгенерированный testY был хуже чем в прошлый раз.\n",
    "\n",
    "#Потом при таких параметрах : \n",
    "# alpha = 0.0431\n",
    "# delta = 0.000000001\n",
    "# l1_coef = 0.1747\n",
    "# n_epoch = 2\n",
    "# batch_size = 8\n",
    "# Точность стала 0.64182 .Она повысилась при увелечении шага градиентного спуска.\n",
    " \n",
    "#Потом при таких параметрах : \n",
    "# alpha = 0.0431\n",
    "# delta = 0.000000001\n",
    "# l1_coef = 0.1747\n",
    "# n_epoch = 2\n",
    "# batch_size = 9\n",
    "# Точность стала 0.75840 .Теперь она повысилась при увелечении батча.    \n",
    "\n",
    "#Потом при таких параметрах : \n",
    "# alpha = 0.0431\n",
    "# delta = 0.000000001\n",
    "# l1_coef = 0.1757\n",
    "# n_epoch = 2\n",
    "# batch_size = 9\n",
    "# Точность стала 0.76670 .Теперь она повысилась при увелечении коэффицента регуляризации.   \n",
    "\n",
    "# Дальше при этих же параметрах точность понизилась и стала 0.69586  \n",
    "# Потом снова на этих параметрах повысилась и стала 0.73779\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
